{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Quantitative Textual Analysis - Week 8: Correlations and Classifications\n",
    "\n",
    "## Brezina 2018 ch. 5: Correlations\n",
    "\n",
    "### Pearson's correlation (pp. 142–146)\n",
    "\n",
    "Pearson's correlation (r) is expressed as follows:\n",
    "\n",
    "```math\n",
    "r = \\frac{\\text{covariance}}{SD_1 \\times SD_2}\n",
    "```\n",
    "\n",
    "Covariance, in turn, is expressed:\n",
    "\n",
    "```math\n",
    "\\text{covariance} = \\frac{\\text{sum of multiplied distances from } mean_1 \\text{ and } mean_2}{\\text{total no. of cases} - 1}\n",
    "```\n",
    "\n",
    "For example, suppose that we have five documents with $N_a$ adjectives and $N_n$ nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [(5, 10), (12, 15), (14, 25), (15, 26), (20, 30)]\n",
    "\n",
    "def covariance(corpus: list[(int, int)]):\n",
    "    mean_1 = sum([a for a, _ in corpus]) / len(corpus)\n",
    "    mean_2 = sum([b for _, b in corpus]) / len(corpus)\n",
    "\n",
    "    return sum([(mean_1 - a) * (mean_2 - b) for a, b in corpus]) / (len(corpus) - 1)\n",
    "\n",
    "docs_covariance = covariance(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the standard deviation for each variable (number of adjectives and number of nouns) using the **sample standard deviation** from Brezina 2018 p. 50:\n",
    "\n",
    "```math\n",
    "\\text{standard deviation}_\\text{sample} = \\sqrt{\\frac{\\text{sum of squared distances from the mean}}{\\text{total no. of cases - 1}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9384978052288936"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def sd_sample(arr: list[int]):\n",
    "    mean = sum(arr) / len(arr)\n",
    "\n",
    "    return math.sqrt(sum([(mean - x)**2 for x in arr]) / (len(arr) - 1))\n",
    "\n",
    "sd_1 = sd_sample([a for a, _ in docs])\n",
    "sd_2 = sd_sample([b for _, b in docs])\n",
    "\n",
    "docs_covariance / (sd_1 * sd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, Pearson's correlation indicates a _very_ strong positive correlation in between the number of adjectives and the number of nouns in these (made-up) documents.\n",
    "\n",
    "Pearson's correlation will always range between -1 and +1: negative numbers indicate a negative correlation, and positive numbers indicate a positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to report with correlation measures\n",
    "\n",
    "As you can probably guess, it's important to report a p-value or confidence interval wiith your correlation statistics in order to give your readers a sense of statistical significance (which, as it turns out, is directly correlated to the number of observations).\n",
    "\n",
    "Note that the functions that we wrote don't care about the length of the input arrays, as long as their respective type signatures are obeyed (a list of 2-tuples of `int`s for `covariance` and a list of `int`s for `sd_sample`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scipy instead of calculating Pearson's _r_ by hand\n",
    "\n",
    "Instead of calculating Pearson's _r_ and associated p-values by hand, we can use the `scipy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=np.float64(0.9384978052288937), pvalue=np.float64(0.018139369943329754))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# zip(*docs) is a python-ism for \"split this list of tuples into separate tuples\"\n",
    "x, y = zip(*docs)\n",
    "\n",
    "stats.pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Lab 1\n",
    "\n",
    "In the `treebanks/` directory, you'll find annotated versions of several of Sophocles' plays. (You don't need to know Greek to complete this exercise.)\n",
    "\n",
    "Each file contains sentence- and word-level annotations. We'll focus on the `word` elements, which look something like this:\n",
    "\n",
    "`<word id=\"12\" form=\"ἐκμάθοις\" lemma=\"ἐκμανθάνω\" postag=\"v2saoa---\" head=\"8\" relation=\"ATR\" cite=\"urn:cts:greekLit:tlg0011.tlg001:2\"/>`\n",
    "\n",
    "The `postag` (\"part-of-speech tag\") attribute will be our focus for this exercise. Its first letter tells us the part of speech for a given word --- `v` for \"verb,\" `a` for \"adjective,\" `n` for \"noun,\" and so on.\n",
    "\n",
    "Using the [`lxml`](https://lxml.de) library, write an xpath expression for parsing the `word` tags and their respective `@postag` attributes.\n",
    "\n",
    "Then determine if the correlation between the number of adjectives and number of nouns in a document that we see in the BNE corpus in Brezina 2018 also holds true for this subset of the Sophoclean corpus.\n",
    "\n",
    "Finally, run similar analyses for correlations between nouns and verbs and between verbs and adjectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis\n",
    "\n",
    "Brezina 2018 (164) defines **factor analysis** as \"a complex mathematical procedure that reduces a large number of linguistic variables. This is done by considering correlations betwen variables...; those that correlate -- both positively and negatively -- are considered components of the same factor because they have a connection.... A **factor** is thus a group of related linguistic variables summarizing a more general tendency ... in the data.\"\n",
    "\n",
    "See Brezina 2018 (165, f. 5.18) for an illustration of the usefulness of factor analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
